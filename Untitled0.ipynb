{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q6HpCe-AJc_r","executionInfo":{"status":"ok","timestamp":1683386637905,"user_tz":-540,"elapsed":24643,"user":{"displayName":"박희민","userId":"08232915071664532971"}},"outputId":"f0a37f41-4348-4beb-a47f-ae26abcf50a7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Colab Notebooks"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7igTM1oAJqiR","executionInfo":{"status":"ok","timestamp":1683386642137,"user_tz":-540,"elapsed":1371,"user":{"displayName":"박희민","userId":"08232915071664532971"}},"outputId":"3dfa96f8-87b9-4a4c-e93a-1a3754ef723a"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks\n"]}]},{"cell_type":"code","source":["import statistics\n","\n","import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from testdataset import PmAwsDataset\n","import numpy as np\n","import copy\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","batch_size = 32\n","num_input = 48\n","\n","# Get cpu or gpu device for training.\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using {device} device\")\n","\n","# Define model\n","class LSTM(nn.Module):\n","    def __init__(self):\n","        super(LSTM, self).__init__()\n","        self.num_layers = num_layers\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.num_classes = num_classes\n","\n","        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n","        self.fc_1 = nn.Linear(hidden_size, 128)\n","        self.fc = nn.Linear(128, num_classes)\n","\n","        self.relu = nn.ReLU()\n","\n","\n","    def forward(self, x):\n","        h_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","        c_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","\n","        output, (hn, cn) = self.lstm(x, (h_0, c_0))\n","\n","        hn = hn.view(-1, self.hidden_size)\n","        out = self.relu(hn)\n","        out = self.fc_1(out)\n","        out = self.relu(out)\n","        out = self.fc(out)\n","        return out[:,0]\n","\n","\n","def train(dataloader, model, loss_fn, optimizer):\n","    size = len(dataloader.dataset)\n","    model.train()\n","    mean_loss = 0\n","    count_loss = 0\n","    for batch, (X, y) in enumerate(dataloader):\n","\n","        X, y = X.to(device), y.to(device)\n","\n","        # Compute prediction error\n","        pred = model(X)\n","        loss = loss_fn(pred, y)#y.reshape((-1,1)))\n","\n","        # Backpropagation\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        mean_loss += loss.item()\n","        count_loss += len(X)\n","\n","    mean_loss /= count_loss\n","    print(f\"mean loss: {mean_loss:>7f}\")\n","\n","def test(dataloader, model, loss_fn, mode='val'):\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    model.eval()\n","    test_loss = 0\n","    count_test = 0\n","    with torch.no_grad():\n","        for X, y in dataloader:\n","            X, y = X.to(device), y.to(device)\n","            pred = model(X)\n","            test_loss += loss_fn(pred, y).item()#.reshape(-1,1)).item()\n","            count_test += len(X)\n","    test_loss /= count_test\n","    test_loss = np.sqrt(test_loss)\n","    if mode == 'val':\n","        print(f\"Validation L1: {test_loss:>7f} \\n\")\n","    else:\n","        print(f\"Test L1: {test_loss:>7f} \\n\")\n","\n","    return test_loss\n","\n","# for t in range(num_repeat):\n","#     np.random.seed(t)\n","#     tf.random.set_seed(t)\n","for nf in range(1,10):\n","\n","\n","    sd_train = PmAwsDataset(\"train\", num_input=num_input,num_file= nf)\n","    sd_val = PmAwsDataset(\"val\", num_input=num_input, num_file = nf)\n","    sd_test = PmAwsDataset(\"test\", num_input=num_input, num_file = nf)\n","\n","    train_dataloader = DataLoader(sd_train, batch_size=batch_size)\n","    val_dataloader = DataLoader(sd_val, batch_size=batch_size)\n","    test_dataloader = DataLoader(sd_test, batch_size=batch_size)\n","\n","    epochs = 1\n","    max_tolerance = 100\n","    input_size = 6\n","    num_layers = 1\n","    hidden_size = 256\n","    num_classes = 1\n","    y_pred_lstm=[]\n","\n","    tolerance = 0\n","    best_val_loss = np.finfo(float).max\n","    if nf == 1:\n","        model = LSTM().to(device)\n","    else:\n","        model = torch.load(path)\n","\n","    loss_fn = nn.L1Loss(reduction=\"sum\")\n","    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n","\n","    for t in range(epochs):\n","        print(f\"Epoch {t+1}\\n-------------------------------\")\n","        train(train_dataloader, model, loss_fn, optimizer)\n","        val_loss = test(val_dataloader, model, loss_fn)\n","        if val_loss > best_val_loss:\n","            if tolerance > max_tolerance:\n","                break\n","            else:\n","                tolerance += 1\n","        else:\n","            best_val_loss = val_loss\n","    path = f'./model.pt'\n","    torch.save(model, path)\n","    print(\"Done!\")\n","\n","    path = f'./model.pt'\n","    net = torch.load(path)\n","\n","    loss_fn = nn.L1Loss(reduction=\"sum\")\n","    test_loss = test(test_dataloader, net, loss_fn, mode='test')\n","\n","    X, y_gt_ = sd_test[0:]\n","    X = torch.Tensor(X).to(device)\n","    y = net(X)\n","    y_gt = y_gt_[0]\n","    y_pred_lstm.append((y).detach().cpu().numpy()[:])\n","    y_pred_lstm = np.array(y_pred_lstm)\n","    y_pred_lstm =  np.ravel(y_pred_lstm, order='C')\n","    print(y_pred_lstm)\n","    print(y_gt_)\n","    plt.scatter(y_gt_, y_pred_lstm)\n","    plt.xlabel(\"True Values\")\n","    plt.ylabel(\"Predictions\")\n","    plt.show()\n","\n","    # Correlation coefficient\n","    correlation_matrix = np.corrcoef(y_gt_, y_pred_lstm)\n","    correlation_coefficient = correlation_matrix[0, 1]\n","    print(\"Correlation coefficient:\", correlation_coefficient)\n","    # px = np.arange(1, 7013)\n","    # py1 = y_pred_lstm\n","    # py2 = y_gt_\n","    # # 하나의 그래프 영역에 두 개의 그래프 그리기\n","    # fig, ax = plt.subplots(figsize=(25, 7))\n","    # ax.plot(px, py1, label='y1')\n","    # ax.plot(px, py2, label='y2')\n","\n","    # # 그래프 제목, 레이블 등 설정하기\n","    # ax.set_title('Comparison of y1 and y2')\n","    # ax.set_xlabel('X axis')\n","    # ax.set_ylabel('Y axis')\n","    # ax.legend()\n","\n","    # # 그래프 보여주기\n","    # plt.show()\n","\n","path = f'./model.pt'\n","model = torch.load(path)\n","model.eval()\n","\n","sd_pred = PmAwsDataset(\"pred\", num_input=num_input)\n","\n","pred_dataloader = DataLoader(sd_pred, batch_size=batch_size)\n","for i in range(3):\n","# for i in range(len(sd_pred)):\n","    X, y = sd_pred[i]\n","    if np.isnan(y):\n","        with torch.no_grad():\n","            tensor_input_data = torch.Tensor(X).to(device)\n","            tensor_input_data = tensor_input_data.unsqueeze(dim=0)\n","            pred_data = model(tensor_input_data)\n","            pred_data = pred_data.detach().cpu().numpy()\n","            print(pred_data)\n","            for j in range(10):\n","                sd_pred[i+j+1][0][47-j][0] = pred_data\n","            print(X)\n","            print(y)\n","        \n","    else:\n","        continue\n","print(sd_pred[5][0][47][0])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":394},"id":"emf98gilJz0k","outputId":"c0ea448a-0680-43dc-b8f6-5a64993524a4","executionInfo":{"status":"error","timestamp":1683386656583,"user_tz":-540,"elapsed":4047,"user":{"displayName":"박희민","userId":"08232915071664532971"}}},"execution_count":4,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-42eb0a381d15>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtestdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPmAwsDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'testdataset'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]}]}